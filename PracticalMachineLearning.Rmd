---
title: "Practical Machine Learning Course Project"
author: "Janina Reyes"
date: "December 5, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Background of the Study

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. A group of enthusiasts took measurements about themselves regularly to improve their health, to find patterns in their behavior. Most people quantify how much of a particular activity they do, but rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The subjects were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
Source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

## Instruction

The goal of the project is to predict the manner how the accelerometer experiment is executed.
In this course project it is expected that the following activities are performed:
- model building
- cross validation
- sample error expectation
- evaluation of models
- prediction of test values

## Preparation of Required Packages
The prerequisite packages are loaded to perform machine learning.

```{r, results='hide'}
install.packages("caret",repos = "http://cran.us.r-project.org")
library(caret)
install.packages("randomForest",repos = "http://cran.us.r-project.org")
library(randomForest)
install.packages("rattle",repos = "http://cran.us.r-project.org")
library(rattle)
library(lattice)
library(ggplot2)
```
## Retrieval and cleaning of data
The csv files are retrieved based on the given url. Columns that contain 90% mostly NA or empty string are removed. IDs are also removed since it is unnecessary for model building. 
```{r}
 trainData <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", header=TRUE) 
 testData <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", header=TRUE)
 almostEmpty <- sapply(trainData, function(x) mean(is.na(x) | x == "")) > 0.90
 trainData <-trainData[,almostEmpty==FALSE]
 almostEmpty2 <- sapply(testData, function(x) mean(is.na(x)| x == "")) > 0.90
 testData <-testData[,almostEmpty2==FALSE]
 trainData<-trainData[,-(1:7)]
 testData<-testData[,-(1:7)]
```

## Data Partitioning
Through the caret package, the training data is partitioned into sixty percent training and forty percent validation set. The test data will be used for verification and prediction of new values
```{r trainData}
 set.seed(1234)
 intrain<- createDataPartition(y= trainData$classe,p=0.6, list = FALSE)
 train <- trainData[intrain,]
 validation <- trainData[-inTrain,]
 
```
## Decision Tree Modeling
The dependent variable identified is classe. rpart is used as the method to be applied on the train data
```{r}
modFit <- train(classe~.,method="rpart",data=train)
print(modFit$finalModel)
```

## Decision Tree Plot

Modeling through Decision Tree plot is shown below.


```{r, echo=FALSE}
fancyRpartPlot(modFit$finalModel)

```

## Prediction of data using Decision Tree

The generated model will be used to predict the values from the validation data for verification. The confusion matrix illustrates whether there are misclassification from the data. The fitted model is also used to predict values from test data
```{r}
 prediction <- predict(modFit,newdata=validation)
 table(prediction)
 print(confusionMatrix(prediction, validation$classe), digits=4)
 ## use prediction model to test data
prediction <- predict(modFit,newdata=testData)
```

The Decision Tree Model has 49.28% accuracy. Another model can be tested whether it would yield higher accuracy.

## Prediction of data using Random Forest

Another machine learning model is Random Forest. Let's compare the performance of the Random Forest versus the Decision Tree. The data is partitioned into 60% training and 40% validation data as well. The model generated is used to predict values of validation and test data.

```{r}
# Set seed for data to be reproducible
set.seed(100);
 trainSet <- sample(nrow(trainData),0.6*nrow(trainData),replace = FALSE)
 train <- trainData[trainSet,]
 valid <- trainData[-trainSet,]
model2 <- randomForest(classe ~ ., data = train, ntree = 500, mtry = 6, importance = TRUE)
model2
 # Predicting on train set
 predTrain <- predict(model2, train, type = "class")
 table(predTrain, train$classe) 
 predValid <- predict(model2, valid, type = "class")
 # Checking classification accuracy
 mean(predValid == valid$classe) 
 table(predValid,valid$classe)
 prediction <- predict(model2,newdata=testData)
 prediction

```

## Importance Plot
The graph illustrates the factors that are considered as important in the model.
```{r, echo = FALSE}
 varImpPlot(model2)
```

The random forest yields higher accuracy percentage of 99.3% as compared to the Decision Tree. The predicted values for the test data are generated.